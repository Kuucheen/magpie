package database

import (
	"context"
	"database/sql"
	"fmt"
	"strings"
	"time"

	"github.com/charmbracelet/log"
	"magpie/internal/api/dto"
	"magpie/internal/config"
	"magpie/internal/support"

	"gorm.io/gorm"
	"gorm.io/gorm/clause"

	"magpie/internal/domain"
)

const scrapeSitesPerPage = 20

// GetScrapingSourcesOfUsers returns all URLs associated with the given user.
func GetScrapingSourcesOfUsers(userID uint) []string {
	var sources []string
	if err := DB.Table("scrape_sites").
		Select("scrape_sites.url").
		Joins("JOIN user_scrape_site uss ON uss.scrape_site_id = scrape_sites.id").
		Where("uss.user_id = ?", userID).
		Order("uss.created_at DESC").
		Scan(&sources).Error; err != nil {
		return nil
	}

	return sources
}

// SaveScrapingSourcesOfUsers appends new sources to the user without removing existing ones.
func SaveScrapingSourcesOfUsers(userID uint, sources []string) ([]domain.ScrapeSite, error) {
	var sites []domain.ScrapeSite
	err := DB.Transaction(func(tx *gorm.DB) error {
		sites = make([]domain.ScrapeSite, 0, len(sources))
		siteIDs := make([]uint64, 0, len(sources))

		// Load the user and existing associations
		var user domain.User
		if err := tx.Preload("ScrapeSites").First(&user, userID).Error; err != nil {
			return err
		}

		existing := make(map[string]struct{}, len(user.ScrapeSites))
		for _, s := range user.ScrapeSites {
			existing[s.URL] = struct{}{}
		}

		seen := make(map[string]struct{}, len(sources))

		// Create or find each ScrapeSite that is not already associated
		for _, raw := range sources {
			trimmed := strings.TrimSpace(raw)
			if trimmed == "" || !support.IsValidURL(trimmed) {
				continue
			}
			if config.IsWebsiteBlocked(trimmed) {
				log.Info("Skipped blocked scrape source", "url", trimmed)
				continue
			}
			if _, ok := existing[trimmed]; ok {
				continue
			}
			if _, ok := seen[trimmed]; ok {
				continue
			}
			seen[trimmed] = struct{}{}

			var site domain.ScrapeSite
			if err := tx.Where("url = ?", trimmed).
				FirstOrCreate(&site, &domain.ScrapeSite{URL: trimmed}).Error; err != nil {
				return err
			}
			sites = append(sites, site)
			siteIDs = append(siteIDs, site.ID)
		}

		// Append only new associations
		if len(sites) > 0 {
			if err := tx.Model(&user).
				Association("ScrapeSites").
				Append(&sites); err != nil {
				return err
			}

			// Reload newly touched sites with Users preloaded
			var loaded []domain.ScrapeSite
			if err := tx.
				Preload("Users", preloadUserIDsOnly).
				Where("id IN ?", siteIDs).
				Find(&loaded).Error; err != nil {
				return err
			}
			sites = loaded
		}

		return nil
	})

	return sites, err
}

func GetAllScrapeSites() ([]domain.ScrapeSite, error) {
	var allProxies []domain.ScrapeSite
	const batchSize = maxParamsPerBatch

	collectedProxies := make([]domain.ScrapeSite, 0)

	err := DB.
		Model(&domain.ScrapeSite{}).
		Distinct("scrape_sites.*").
		Joins("JOIN user_scrape_site uss ON uss.scrape_site_id = scrape_sites.id").
		Preload("Users", preloadUserIDsOnly).
		Order("scrape_sites.id").
		FindInBatches(&allProxies, batchSize, func(tx *gorm.DB, batch int) error {
			collectedProxies = append(collectedProxies, allProxies...)
			return nil
		})

	if err.Error != nil {
		return nil, err.Error
	}

	return collectedProxies, nil
}

func AssociateProxiesToScrapeSite(siteID uint64, proxies []domain.Proxy) error {
	if len(proxies) == 0 {
		return nil
	}

	// ProxyScrapeSite inserts touch proxy_id, scrape_site_id and created_at columns.
	const paramsPerRow = 3
	chunkSize := maxParamsPerBatch / paramsPerRow
	if chunkSize > len(proxies) {
		chunkSize = len(proxies)
	}

	return DB.Transaction(func(tx *gorm.DB) error {
		for start := 0; start < len(proxies); start += chunkSize {
			end := start + chunkSize
			if end > len(proxies) {
				end = len(proxies)
			}

			assoc := make([]domain.ProxyScrapeSite, 0, end-start)
			for _, p := range proxies[start:end] {
				if p.ID == 0 {
					continue
				}
				assoc = append(assoc, domain.ProxyScrapeSite{
					ProxyID:      p.ID,
					ScrapeSiteID: siteID,
				})
			}

			if len(assoc) == 0 {
				continue
			}

			if err := tx.
				Clauses(clause.OnConflict{
					Columns:   []clause.Column{{Name: "proxy_id"}, {Name: "scrape_site_id"}},
					DoNothing: true,
				}).
				Create(&assoc).Error; err != nil {
				return err
			}
		}

		return nil
	})
}

func GetAllScrapeSiteCountOfUser(userId uint) int64 {
	var count int64
	DB.Model(&domain.ScrapeSite{}).
		Joins(
			"JOIN user_scrape_site uss ON uss.scrape_site_id = scrape_sites.id AND uss.user_id = ?",
			userId,
		).
		Count(&count)
	return count
}

func GetScrapeSiteInfoPage(userId uint, page int) []dto.ScrapeSiteInfo {
	offset := (page - 1) * scrapeSitesPerPage

	var results []dto.ScrapeSiteInfo

	query := buildScrapeSiteInfoQuery(userId).Select(
		"scrape_sites.id         AS id, " +
			"scrape_sites.url        AS url, " +
			"COALESCE(pc.proxy_count, 0) AS proxy_count, " +
			"COALESCE(ps.alive_count, 0) AS alive_count, " +
			"COALESCE(ps.dead_count, 0) AS dead_count, " +
			"COALESCE(ps.unknown_count, 0) AS unknown_count, " +
			"uss.created_at          AS added_at",
	)

	query.Order("uss.created_at DESC").
		Offset(offset).
		Limit(scrapeSitesPerPage).
		Scan(&results)

	return results
}

func buildScrapeSiteInfoQuery(userId uint) *gorm.DB {
	// subquery: for each scrape_site_id, count only the proxies that this user has
	subQuery := DB.
		Model(&domain.ProxyScrapeSite{}).
		Select("scrape_site_id, COUNT(*) AS proxy_count").
		Joins("JOIN user_proxies up ON up.proxy_id = proxy_scrape_site.proxy_id AND up.user_id = ?", userId).
		Group("scrape_site_id")

	statusQuery := DB.
		Table("proxy_scrape_site pss").
		Select(
			"pss.scrape_site_id AS scrape_site_id, "+
				"COALESCE(SUM(CASE WHEN pos.overall_alive IS TRUE THEN 1 ELSE 0 END), 0) AS alive_count, "+
				"COALESCE(SUM(CASE WHEN pos.overall_alive IS FALSE THEN 1 ELSE 0 END), 0) AS dead_count, "+
				"COALESCE(SUM(CASE WHEN pos.proxy_id IS NULL THEN 1 ELSE 0 END), 0) AS unknown_count",
		).
		Joins("JOIN user_proxies up ON up.proxy_id = pss.proxy_id AND up.user_id = ?", userId).
		Joins("LEFT JOIN proxy_overall_statuses pos ON pos.proxy_id = pss.proxy_id").
		Group("pss.scrape_site_id")

	return DB.
		Model(&domain.ScrapeSite{}).
		// only the sites this user has added
		Joins("JOIN user_scrape_site uss ON uss.scrape_site_id = scrape_sites.id AND uss.user_id = ?", userId).
		// attach the per-site, per-user proxy counts
		Joins("LEFT JOIN (?) AS pc ON pc.scrape_site_id = scrape_sites.id", subQuery).
		Joins("LEFT JOIN (?) AS ps ON ps.scrape_site_id = scrape_sites.id", statusQuery)
}

type scrapeSiteAggregateRow struct {
	ProxyCount       uint            `gorm:"column:proxy_count"`
	AliveCount       uint            `gorm:"column:alive_count"`
	DeadCount        uint            `gorm:"column:dead_count"`
	UnknownCount     uint            `gorm:"column:unknown_count"`
	AvgReputation    sql.NullFloat64 `gorm:"column:avg_reputation"`
	LastProxyAddedAt sql.NullTime    `gorm:"column:last_proxy_added_at"`
	LastCheckedAt    sql.NullTime    `gorm:"column:last_checked_at"`
}

func GetScrapeSiteDetail(userId uint, scrapeSiteId uint64) (*dto.ScrapeSiteDetail, error) {
	if scrapeSiteId == 0 {
		return nil, nil
	}

	type scrapeSiteBaseRow struct {
		Id      uint64    `gorm:"column:id"`
		Url     string    `gorm:"column:url"`
		AddedAt time.Time `gorm:"column:added_at"`
	}

	var base scrapeSiteBaseRow
	baseResult := DB.Model(&domain.ScrapeSite{}).
		Select(
			"scrape_sites.id AS id, "+
				"scrape_sites.url AS url, "+
				"uss.created_at AS added_at",
		).
		Joins("JOIN user_scrape_site uss ON uss.scrape_site_id = scrape_sites.id AND uss.user_id = ?", userId).
		Where("scrape_sites.id = ?", scrapeSiteId).
		Limit(1).
		Scan(&base)
	if baseResult.Error != nil {
		return nil, baseResult.Error
	}
	if baseResult.RowsAffected == 0 {
		return nil, nil
	}

	var stats scrapeSiteAggregateRow
	statsResult := DB.Table("proxy_scrape_site pss").
		Select(
			"COUNT(DISTINCT pss.proxy_id) AS proxy_count, "+
				"COALESCE(SUM(CASE WHEN pos.overall_alive IS TRUE THEN 1 ELSE 0 END), 0) AS alive_count, "+
				"COALESCE(SUM(CASE WHEN pos.overall_alive IS FALSE THEN 1 ELSE 0 END), 0) AS dead_count, "+
				"COALESCE(SUM(CASE WHEN pos.proxy_id IS NULL THEN 1 ELSE 0 END), 0) AS unknown_count, "+
				"AVG(pr.score) AS avg_reputation, "+
				"MAX(pss.created_at) AS last_proxy_added_at, "+
				"MAX(pos.last_checked_at) AS last_checked_at",
		).
		Joins("JOIN user_proxies up ON up.proxy_id = pss.proxy_id AND up.user_id = ?", userId).
		Joins("LEFT JOIN proxy_overall_statuses pos ON pos.proxy_id = pss.proxy_id").
		Joins("LEFT JOIN proxy_reputations pr ON pr.proxy_id = pss.proxy_id AND pr.kind = ?", domain.ProxyReputationKindOverall).
		Where("pss.scrape_site_id = ?", scrapeSiteId).
		Scan(&stats)
	if statsResult.Error != nil {
		return nil, statsResult.Error
	}

	type reputationCount struct {
		Label string `gorm:"column:label"`
		Count uint   `gorm:"column:count"`
	}

	var repCounts []reputationCount
	repResult := DB.Table("proxy_scrape_site pss").
		Select("LOWER(COALESCE(NULLIF(pr.label, ''), 'unknown')) AS label, COUNT(*) AS count").
		Joins("JOIN user_proxies up ON up.proxy_id = pss.proxy_id AND up.user_id = ?", userId).
		Joins("LEFT JOIN proxy_reputations pr ON pr.proxy_id = pss.proxy_id AND pr.kind = ?", domain.ProxyReputationKindOverall).
		Where("pss.scrape_site_id = ?", scrapeSiteId).
		Group("label").
		Scan(&repCounts)
	if repResult.Error != nil {
		return nil, repResult.Error
	}

	breakdown := dto.ScrapeSiteReputationBreakdown{}
	for _, row := range repCounts {
		switch row.Label {
		case "good":
			breakdown.Good = row.Count
		case "neutral":
			breakdown.Neutral = row.Count
		case "poor":
			breakdown.Poor = row.Count
		default:
			breakdown.Unknown += row.Count
		}
	}

	var avgReputation *float32
	if stats.AvgReputation.Valid {
		value := float32(stats.AvgReputation.Float64)
		avgReputation = &value
	}

	var lastProxyAddedAt *time.Time
	if stats.LastProxyAddedAt.Valid {
		value := stats.LastProxyAddedAt.Time
		lastProxyAddedAt = &value
	}

	var lastCheckedAt *time.Time
	if stats.LastCheckedAt.Valid {
		value := stats.LastCheckedAt.Time
		lastCheckedAt = &value
	}

	detail := &dto.ScrapeSiteDetail{
		Id:                  base.Id,
		Url:                 base.Url,
		AddedAt:             base.AddedAt,
		ProxyCount:          stats.ProxyCount,
		AliveCount:          stats.AliveCount,
		DeadCount:           stats.DeadCount,
		UnknownCount:        stats.UnknownCount,
		AvgReputation:       avgReputation,
		LastProxyAddedAt:    lastProxyAddedAt,
		LastCheckedAt:       lastCheckedAt,
		ReputationBreakdown: breakdown,
	}

	return detail, nil
}

func GetScrapeSiteProxyPage(userId uint, scrapeSiteId uint64, page int, pageSize int, search string, filters dto.ProxyListFilters) ([]dto.ProxyInfo, int64, error) {
	if scrapeSiteId == 0 {
		return []dto.ProxyInfo{}, 0, nil
	}
	if page < 1 {
		page = 1
	}
	if pageSize <= 0 || pageSize > maxProxiesPerPage {
		pageSize = proxiesPerPage
	}

	subQuery := DB.Model(&domain.ProxyStatistic{}).
		Select("DISTINCT ON (proxy_id) *").
		Order("proxy_id, created_at DESC, id DESC")
	healthSubQuery := buildProxyHealthSubQuery(userId)

	query := DB.Model(&domain.Proxy{}).
		Select(
			"proxies.id AS id, "+
				"proxies.ip AS ip_encrypted, "+
				"proxies.port AS port, "+
				"COALESCE(NULLIF(proxies.estimated_type, ''), 'N/A') AS estimated_type, "+
				"COALESCE(ps.response_time, 0) AS response_time, "+
				"COALESCE(NULLIF(proxies.country, ''), 'N/A') AS country, "+
				"COALESCE(al.name, 'N/A') AS anonymity_level, "+
				"COALESCE(pos.overall_alive, false) AS alive, "+
				"stats.health_overall AS health_overall, "+
				"stats.health_http AS health_http, "+
				"stats.health_https AS health_https, "+
				"stats.health_socks4 AS health_socks4, "+
				"stats.health_socks5 AS health_socks5, "+
				"COALESCE(pos.last_checked_at, ps.created_at, '0001-01-01 00:00:00'::timestamp) AS latest_check",
		).
		Joins("JOIN user_proxies up ON up.proxy_id = proxies.id AND up.user_id = ?", userId).
		Joins("JOIN proxy_scrape_site pss ON pss.proxy_id = proxies.id AND pss.scrape_site_id = ?", scrapeSiteId).
		Joins("JOIN user_scrape_site uss ON uss.scrape_site_id = pss.scrape_site_id AND uss.user_id = ?", userId).
		Joins("LEFT JOIN (?) AS ps ON ps.proxy_id = proxies.id", subQuery).
		Joins("LEFT JOIN proxy_overall_statuses pos ON pos.proxy_id = proxies.id").
		Joins("LEFT JOIN (?) AS stats ON stats.proxy_id = proxies.id", healthSubQuery).
		Joins("LEFT JOIN anonymity_levels al ON al.id = ps.level_id").
		Order("alive DESC, latest_check DESC")

	filterQuery := buildProxyListFilterQuery(userId, filters)
	if filterQuery != nil {
		query = query.Where("proxies.id IN (?)", filterQuery)
	}

	rows := make([]dto.ProxyInfoRow, 0)
	normalizedSearch := strings.TrimSpace(search)

	if normalizedSearch == "" {
		offset := (page - 1) * pageSize
		query = query.Offset(offset).Limit(pageSize)
		if err := query.Scan(&rows).Error; err != nil {
			return []dto.ProxyInfo{}, 0, err
		}

		proxies := proxyInfoRowsToDTO(rows)
		attachReputationsToProxyInfos(proxies)

		var total int64
		countQuery := DB.Model(&domain.Proxy{}).
			Joins("JOIN user_proxies up ON up.proxy_id = proxies.id AND up.user_id = ?", userId).
			Joins("JOIN proxy_scrape_site pss ON pss.proxy_id = proxies.id AND pss.scrape_site_id = ?", scrapeSiteId).
			Joins("JOIN user_scrape_site uss ON uss.scrape_site_id = pss.scrape_site_id AND uss.user_id = ?", userId)
		if filterQuery != nil {
			countQuery = countQuery.Where("proxies.id IN (?)", filterQuery)
		}
		if err := countQuery.Distinct("proxies.id").Count(&total).Error; err != nil {
			return proxies, 0, err
		}

		return proxies, total, nil
	}

	if err := query.Scan(&rows).Error; err != nil {
		return []dto.ProxyInfo{}, 0, err
	}

	proxies := proxyInfoRowsToDTO(rows)
	attachReputationsToProxyInfos(proxies)
	filtered := filterProxiesBySearch(proxies, normalizedSearch)
	total := int64(len(filtered))
	start := (page - 1) * pageSize
	if start >= len(filtered) {
		return []dto.ProxyInfo{}, total, nil
	}

	end := start + pageSize
	if end > len(filtered) {
		end = len(filtered)
	}

	pageSlice := filtered[start:end]
	attachReputationsToProxyInfos(pageSlice)

	return pageSlice, total, nil
}

func DeleteScrapeSiteRelation(userId uint, scrapeSite []int) (int64, []domain.ScrapeSite, error) {
	if len(scrapeSite) == 0 {
		return 0, nil, nil
	}

	chunkSize := deleteChunkSize
	if chunkSize > len(scrapeSite) {
		chunkSize = len(scrapeSite)
	}
	if chunkSize <= 0 {
		chunkSize = len(scrapeSite)
	}

	var totalDeleted int64
	orphanSet := make(map[uint64]struct{})

	for start := 0; start < len(scrapeSite); start += chunkSize {
		end := start + chunkSize
		if end > len(scrapeSite) {
			end = len(scrapeSite)
		}

		chunk := scrapeSite[start:end]
		result := DB.
			Where("scrape_site_id IN ?", chunk).
			Where("user_id = ?", userId).
			Delete(&domain.UserScrapeSite{})

		if result.Error != nil {
			return totalDeleted, nil, result.Error
		}

		totalDeleted += result.RowsAffected

		orphanIDs, err := collectOrphanScrapeSiteIDs(chunk)
		if err != nil {
			return totalDeleted, nil, err
		}

		for _, id := range orphanIDs {
			orphanSet[id] = struct{}{}
		}
	}

	if len(orphanSet) == 0 {
		return totalDeleted, nil, nil
	}

	uniqueIDs := make([]uint64, 0, len(orphanSet))
	for id := range orphanSet {
		uniqueIDs = append(uniqueIDs, id)
	}

	var orphans []domain.ScrapeSite
	if err := DB.Where("id IN ?", uniqueIDs).Find(&orphans).Error; err != nil {
		return totalDeleted, nil, err
	}

	return totalDeleted, orphans, nil
}

func collectOrphanScrapeSiteIDs(candidateIDs []int) ([]uint64, error) {
	if len(candidateIDs) == 0 {
		return nil, nil
	}

	var stillInUse []int
	if err := DB.Model(&domain.UserScrapeSite{}).
		Where("scrape_site_id IN ?", candidateIDs).
		Distinct("scrape_site_id").
		Pluck("scrape_site_id", &stillInUse).Error; err != nil {
		return nil, err
	}

	inUseSet := make(map[int]struct{}, len(stillInUse))
	for _, id := range stillInUse {
		inUseSet[id] = struct{}{}
	}

	seen := make(map[int]struct{}, len(candidateIDs))
	orphanIDs := make([]uint64, 0, len(candidateIDs))
	for _, candidate := range candidateIDs {
		if _, alreadySeen := seen[candidate]; alreadySeen {
			continue
		}
		seen[candidate] = struct{}{}

		if _, ok := inUseSet[candidate]; ok {
			continue
		}

		orphanIDs = append(orphanIDs, uint64(candidate))
	}

	if len(orphanIDs) == 0 {
		return nil, nil
	}

	return orphanIDs, nil
}

func DeleteOrphanScrapeSites(ctx context.Context) (int64, error) {
	if DB == nil {
		return 0, fmt.Errorf("database not initialised")
	}

	db := DB
	if ctx != nil {
		db = db.WithContext(ctx)
	}

	result := db.
		Where("NOT EXISTS (SELECT 1 FROM user_scrape_site us WHERE us.scrape_site_id = scrape_sites.id)").
		Delete(&domain.ScrapeSite{})
	if result.Error != nil {
		return 0, result.Error
	}

	return result.RowsAffected, nil
}

func ScrapeSiteHasUsers(siteID uint64) (bool, error) {
	var count int64
	if err := DB.Model(&domain.UserScrapeSite{}).
		Where("scrape_site_id = ?", siteID).
		Count(&count).Error; err != nil {
		return false, err
	}
	return count > 0, nil
}
